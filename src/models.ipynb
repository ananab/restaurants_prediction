{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import matplotlib.cm as cm\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "air_reserve=pd.read_csv('../../data2/air_reserve.csv')\n",
    "air_store=pd.read_csv('../../data2/air_store_info.csv')\n",
    "air_visit=pd.read_csv('../../data2/air_visit_data.csv')\n",
    "dates=pd.read_csv('../../data2/date_info.csv')\n",
    "hpg_reserve=pd.read_csv('../../data2/hpg_reserve.csv')\n",
    "hpg_store=pd.read_csv('../../data2/hpg_store_info.csv')\n",
    "sample_sub=pd.read_csv('../../data2/sample_submission.csv')\n",
    "store_relation=pd.read_csv('../../data2/store_id_relation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 92378 entries, 0 to 92377\n",
      "Data columns (total 4 columns):\n",
      "air_store_id        92378 non-null object\n",
      "visit_datetime      92378 non-null object\n",
      "reserve_datetime    92378 non-null object\n",
      "reserve_visitors    92378 non-null int64\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "air_reserve.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7feae42d5c88>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feae47947b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(air_reserve.groupby('air_store_id').agg('mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 829 entries, 0 to 828\n",
      "Data columns (total 5 columns):\n",
      "air_store_id      829 non-null object\n",
      "air_genre_name    829 non-null object\n",
      "air_area_name     829 non-null object\n",
      "latitude          829 non-null float64\n",
      "longitude         829 non-null float64\n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 32.5+ KB\n"
     ]
    }
   ],
   "source": [
    "air_store.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 252108 entries, 0 to 252107\n",
      "Data columns (total 3 columns):\n",
      "air_store_id    252108 non-null object\n",
      "visit_date      252108 non-null object\n",
      "visitors        252108 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 5.8+ MB\n"
     ]
    }
   ],
   "source": [
    "air_visit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaih/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/kaih/.local/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/home/kaih/.local/lib/python3.5/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Contributions from:\n",
    "DSEverything - Mean Mix - Math, Geo, Harmonic (LB 0.493) \n",
    "https://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\n",
    "JdPaletto - Surprised Yet? - Part2 - (LB: 0.503)\n",
    "https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503\n",
    "hklee - weighted mean comparisons, LB 0.497, 1ST\n",
    "https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st\n",
    "\n",
    "Also all comments for changes, encouragement, and forked scripts rock\n",
    "\n",
    "Keep the Surprise Going\n",
    "\"\"\"\n",
    "\n",
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from keras.layers import Embedding, Input, Dense\n",
    "from keras.models import Model\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#\"START HKLEE FEATURE\"\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../../data2/*.csv')}\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(lambda x: (x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1, axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) \n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=['air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), how='left')['visitors_y'].values\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "test_visit_var = sample_submission.visitors.map(pd.np.expm1)\n",
    "\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../data2/air_visit_data.csv')\n",
    "    }\n",
    "\n",
    "data['tra'] = data['tra'].merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "data['tra'] = data['tra'].merge(visitors, on=['air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = data['tra'].visitors_y.isnull()\n",
    "data['tra'].loc[missings, 'visitors_y'] = data['tra'][missings].merge(visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), how='left')['visitors_y'].values\n",
    "\n",
    "missings = data['tra'].visitors_y.isnull()\n",
    "data['tra'].loc[missings, 'visitors_y'] = data['tra'][missings].merge(visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "train_visit_var = data['tra'].visitors_y.map(pd.np.expm1)\n",
    "#\"END HKLEE FEATURE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test data prepared\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = {\n",
    "    'tra': pd.read_csv('../../data2/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../../data2/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../../data2/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../../data2/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../../data2/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../../data2/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../../data2/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../../data2/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }\n",
    "\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime']).dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime']).dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "#OPTIMIZED BY JEROME VALLET\n",
    "tmp = data['tra'].groupby(['air_store_id','dow']).agg({'visitors' : [np.min,np.mean,np.median,np.max,np.size]}).reset_index()\n",
    "tmp.columns = ['air_store_id', 'dow', 'min_visitors', 'mean_visitors', 'median_visitors','max_visitors','count_observations']\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "#sure it can be compressed...\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n",
    "\n",
    "train = pd.merge(train, stores, how='inner', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n",
    "\n",
    "# NEW FEATURES FROM JMBULL\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred)**0.5\n",
    "\n",
    "value_col = ['holiday_flg','min_visitors','mean_visitors','median_visitors','max_visitors','count_observations',\n",
    "'rs1_x','rv1_x','rs2_x','rv2_x','rs1_y','rv1_y','rs2_y','rv2_y','total_reserv_sum','total_reserv_mean',\n",
    "'total_reserv_dt_diff_mean','date_int','var_max_lat','var_max_long','lon_plus_lat']\n",
    "\n",
    "nn_col = value_col + ['dow', 'year', 'month', 'air_store_id2', 'air_area_name', 'air_genre_name',\n",
    "'air_area_name0', 'air_area_name1', 'air_area_name2', 'air_area_name3', 'air_area_name4',\n",
    "'air_area_name5', 'air_area_name6', 'air_genre_name0', 'air_genre_name1',\n",
    "'air_genre_name2', 'air_genre_name3', 'air_genre_name4']\n",
    "\n",
    "\n",
    "X = train.copy()\n",
    "X_test = test[nn_col].copy()\n",
    "\n",
    "value_scaler = preprocessing.MinMaxScaler()\n",
    "for vcol in value_col:\n",
    "    X[vcol] = value_scaler.fit_transform(X[vcol].values.astype(np.float64).reshape(-1, 1))\n",
    "    X_test[vcol] = value_scaler.transform(X_test[vcol].values.astype(np.float64).reshape(-1, 1))\n",
    "\n",
    "X_train = list(X[nn_col].T.as_matrix())\n",
    "Y_train = np.log1p(X['visitors']).values\n",
    "nn_train = [X_train, Y_train]\n",
    "nn_test = [list(X_test[nn_col].T.as_matrix())]\n",
    "print(\"Train and test data prepared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nn_complete_model(train, hidden1_neurons=35, hidden2_neurons=15):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        train:           train dataframe(used to define the input size of the embedding layer)\n",
    "        hidden1_neurons: number of neurons in the first hidden layer\n",
    "        hidden2_neurons: number of neurons in the first hidden layer\n",
    "    Output:\n",
    "        return 'keras neural network model'\n",
    "    \"\"\"\n",
    "    K.clear_session()\n",
    "\n",
    "    air_store_id = Input(shape=(1,), dtype='int32', name='air_store_id')\n",
    "    air_store_id_emb = Embedding(len(train['air_store_id2'].unique()) + 1, 15, input_shape=(1,),\n",
    "                                 name='air_store_id_emb')(air_store_id)\n",
    "    air_store_id_emb = keras.layers.Flatten(name='air_store_id_emb_flatten')(air_store_id_emb)\n",
    "\n",
    "    dow = Input(shape=(1,), dtype='int32', name='dow')\n",
    "    dow_emb = Embedding(8, 3, input_shape=(1,), name='dow_emb')(dow)\n",
    "    dow_emb = keras.layers.Flatten(name='dow_emb_flatten')(dow_emb)\n",
    "\n",
    "    month = Input(shape=(1,), dtype='int32', name='month')\n",
    "    month_emb = Embedding(13, 3, input_shape=(1,), name='month_emb')(month)\n",
    "    month_emb = keras.layers.Flatten(name='month_emb_flatten')(month_emb)\n",
    "\n",
    "    air_area_name, air_genre_name = [], []\n",
    "    air_area_name_emb, air_genre_name_emb = [], []\n",
    "    for i in range(7):\n",
    "        area_name_col = 'air_area_name' + str(i)\n",
    "        air_area_name.append(Input(shape=(1,), dtype='int32', name=area_name_col))\n",
    "        tmp = Embedding(len(train[area_name_col].unique()), 3, input_shape=(1,),\n",
    "                        name=area_name_col + '_emb')(air_area_name[-1])\n",
    "        tmp = keras.layers.Flatten(name=area_name_col + '_emb_flatten')(tmp)\n",
    "        air_area_name_emb.append(tmp)\n",
    "\n",
    "        if i > 4:\n",
    "            continue\n",
    "        area_genre_col = 'air_genre_name' + str(i)\n",
    "        air_genre_name.append(Input(shape=(1,), dtype='int32', name=area_genre_col))\n",
    "        tmp = Embedding(len(train[area_genre_col].unique()), 3, input_shape=(1,),\n",
    "                        name=area_genre_col + '_emb')(air_genre_name[-1])\n",
    "        tmp = keras.layers.Flatten(name=area_genre_col + '_emb_flatten')(tmp)\n",
    "        air_genre_name_emb.append(tmp)\n",
    "\n",
    "    air_genre_name_emb = keras.layers.concatenate(air_genre_name_emb)\n",
    "    air_genre_name_emb = Dense(4, activation='sigmoid', name='final_air_genre_emb')(air_genre_name_emb)\n",
    "\n",
    "    air_area_name_emb = keras.layers.concatenate(air_area_name_emb)\n",
    "    air_area_name_emb = Dense(4, activation='sigmoid', name='final_air_area_emb')(air_area_name_emb)\n",
    "    \n",
    "    air_area_code = Input(shape=(1,), dtype='int32', name='air_area_code')\n",
    "    air_area_code_emb = Embedding(len(train['air_area_name'].unique()), 8, input_shape=(1,), name='air_area_code_emb')(air_area_code)\n",
    "    air_area_code_emb = keras.layers.Flatten(name='air_area_code_emb_flatten')(air_area_code_emb)\n",
    "    \n",
    "    air_genre_code = Input(shape=(1,), dtype='int32', name='air_genre_code')\n",
    "    air_genre_code_emb = Embedding(len(train['air_genre_name'].unique()), 5, input_shape=(1,),\n",
    "                                   name='air_genre_code_emb')(air_genre_code)\n",
    "    air_genre_code_emb = keras.layers.Flatten(name='air_genre_code_emb_flatten')(air_genre_code_emb)\n",
    "\n",
    "    \n",
    "    holiday_flg = Input(shape=(1,), dtype='float32', name='holiday_flg')\n",
    "    year = Input(shape=(1,), dtype='float32', name='year')\n",
    "    min_visitors = Input(shape=(1,), dtype='float32', name='min_visitors')\n",
    "    mean_visitors = Input(shape=(1,), dtype='float32', name='mean_visitors')\n",
    "    median_visitors = Input(shape=(1,), dtype='float32', name='median_visitors')\n",
    "    max_visitors = Input(shape=(1,), dtype='float32', name='max_visitors')\n",
    "    count_observations = Input(shape=(1,), dtype='float32', name='count_observations')\n",
    "    rs1_x = Input(shape=(1,), dtype='float32', name='rs1_x')\n",
    "    rv1_x = Input(shape=(1,), dtype='float32', name='rv1_x')\n",
    "    rs2_x = Input(shape=(1,), dtype='float32', name='rs2_x')\n",
    "    rv2_x = Input(shape=(1,), dtype='float32', name='rv2_x')\n",
    "    rs1_y = Input(shape=(1,), dtype='float32', name='rs1_y')\n",
    "    rv1_y = Input(shape=(1,), dtype='float32', name='rv1_y')\n",
    "    rs2_y = Input(shape=(1,), dtype='float32', name='rs2_y')\n",
    "    rv2_y = Input(shape=(1,), dtype='float32', name='rv2_y')\n",
    "    total_reserv_sum = Input(shape=(1,), dtype='float32', name='total_reserv_sum')\n",
    "    total_reserv_mean = Input(shape=(1,), dtype='float32', name='total_reserv_mean')\n",
    "    total_reserv_dt_diff_mean = Input(shape=(1,), dtype='float32', name='total_reserv_dt_diff_mean')\n",
    "    date_int = Input(shape=(1,), dtype='float32', name='date_int')\n",
    "    var_max_lat = Input(shape=(1,), dtype='float32', name='var_max_lat')\n",
    "    var_max_long = Input(shape=(1,), dtype='float32', name='var_max_long')\n",
    "    lon_plus_lat = Input(shape=(1,), dtype='float32', name='lon_plus_lat')\n",
    "\n",
    "    date_emb = keras.layers.concatenate([dow_emb, month_emb, year, holiday_flg])\n",
    "    date_emb = Dense(5, activation='sigmoid', name='date_merged_emb')(date_emb)\n",
    "\n",
    "    cat_layer = keras.layers.concatenate([holiday_flg, min_visitors, mean_visitors,\n",
    "                    median_visitors, max_visitors, count_observations, rs1_x, rv1_x,\n",
    "                    rs2_x, rv2_x, rs1_y, rv1_y, rs2_y, rv2_y,\n",
    "                    total_reserv_sum, total_reserv_mean, total_reserv_dt_diff_mean,\n",
    "                    date_int, var_max_lat, var_max_long, lon_plus_lat,\n",
    "                    date_emb, air_area_name_emb, air_genre_name_emb,\n",
    "                    air_area_code_emb, air_genre_code_emb, air_store_id_emb])\n",
    "\n",
    "    m = Dense(hidden1_neurons, name='hidden1',\n",
    "             kernel_initializer=keras.initializers.RandomNormal(mean=0.0,\n",
    "                            stddev=0.05, seed=None))(cat_layer)\n",
    "    m = keras.layers.LeakyReLU(alpha=0.2)(m)\n",
    "    m = keras.layers.BatchNormalization()(m)\n",
    "    \n",
    "    m1 = Dense(hidden2_neurons, name='hidden2')(m)\n",
    "    m1 = keras.layers.LeakyReLU(alpha=0.2)(m1)\n",
    "    m = Dense(1, activation='relu')(m1)\n",
    "\n",
    "    inp_ten = [\n",
    "        holiday_flg, min_visitors, mean_visitors, median_visitors, max_visitors, count_observations,\n",
    "        rs1_x, rv1_x, rs2_x, rv2_x, rs1_y, rv1_y, rs2_y, rv2_y, total_reserv_sum, total_reserv_mean,\n",
    "        total_reserv_dt_diff_mean, date_int, var_max_lat, var_max_long, lon_plus_lat,\n",
    "        dow, year, month, air_store_id, air_area_code, air_genre_code\n",
    "    ]\n",
    "    inp_ten += air_area_name\n",
    "    inp_ten += air_genre_name\n",
    "    model = Model(inp_ten, m)\n",
    "    model.compile(loss='mse', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model1 = ensemble.GradientBoostingRegressor(learning_rate=0.2, random_state=3, n_estimators=200, \n",
    "#                                             subsample=0.8, max_depth =10)\n",
    "# model2 = neighbors.KNeighborsRegressor(n_jobs=-1, n_neighbors=4)\n",
    "\n",
    "# model3 = XGBRegressor(learning_rate=0.2, n_estimators=200, subsample=0.8, colsample_bytree=0.8, max_depth =10)\n",
    "# model5 = ensemble.RandomForestRegressor(n_estimators=200,max_depth=12,n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 4s - loss: 0.2697 - acc: 0.0000e+00 - val_loss: 0.2790 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 4s - loss: 0.2652 - acc: 0.0000e+00 - val_loss: 0.3043 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 4s - loss: 0.2616 - acc: 0.0000e+00 - val_loss: 0.2950 - val_acc: 0.0000e+00\n",
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 4s - loss: 0.2465 - acc: 0.0000e+00 - val_loss: 0.2675 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 4s - loss: 0.2461 - acc: 0.0000e+00 - val_loss: 0.2426 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 3s - loss: 0.2460 - acc: 0.0000e+00 - val_loss: 0.2641 - val_acc: 0.0000e+00\n",
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 4s - loss: 0.2409 - acc: 0.0000e+00 - val_loss: 0.2418 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 3s - loss: 0.2409 - acc: 0.0000e+00 - val_loss: 0.2706 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 3s - loss: 0.2403 - acc: 0.0000e+00 - val_loss: 0.2416 - val_acc: 0.0000e+00\n",
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 3s - loss: 0.2375 - acc: 0.0000e+00 - val_loss: 0.2407 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 3s - loss: 0.2375 - acc: 0.0000e+00 - val_loss: 0.2531 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 3s - loss: 0.2376 - acc: 0.0000e+00 - val_loss: 0.2395 - val_acc: 0.0000e+00\n",
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 4s - loss: 0.2358 - acc: 0.0000e+00 - val_loss: 0.2393 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 5s - loss: 0.2356 - acc: 0.0000e+00 - val_loss: 0.2364 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 4s - loss: 0.2353 - acc: 0.0000e+00 - val_loss: 0.2550 - val_acc: 0.0000e+00\n",
      "Model4 trained\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Bojan Tunguz / Surprise Me 2!\n",
    "model4 = get_nn_complete_model(train, hidden2_neurons=12)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    model4.fit(nn_train[0], nn_train[1], epochs=2, verbose=0, batch_size=512, shuffle=True)\n",
    "    model4.fit(nn_train[0], nn_train[1], epochs=3, verbose=1, batch_size=256, shuffle=True, validation_split=0.15)\n",
    "    model4.fit(nn_train[0], nn_train[1], epochs=8, verbose=0, batch_size=256, shuffle=True)\n",
    "print(\"Model4 trained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=12,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=3,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model1.fit(train[col], np.log1p(train['visitors'].values))\n",
    "# model2.fit(train[col], np.log1p(train['visitors'].values))\n",
    "# model3.fit(train[col], np.log1p(train['visitors'].values))\n",
    "# model5.fit(train[col], np.log1p(train['visitors'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = pickle.load(open('model1.pk','rb'))\n",
    "model2 = pickle.load(open('model2.pk','rb'))\n",
    "model3 = pickle.load(open('model3.pk','rb'))\n",
    "model5 = pickle.load(open('model5.pk','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle.dump(model1, open( \"model1.pk\", \"wb\" ) )\n",
    "# pickle.dump(model2, open( \"model2.pk\", \"wb\" ) )\n",
    "# pickle.dump(model3, open( \"model3.pk\", \"wb\" ) )\n",
    "# # pickle.dump(model4, open( \"model4.pk\", \"wb\" ) )\n",
    "# pickle.dump(model5, open( \"model5.pk\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# preds1 = model1.predict(train[col])\n",
    "# preds2 = model2.predict(train[col])\n",
    "# preds3 = model3.predict(train[col])\n",
    "# preds5 = model5.predict(train[col])\n",
    "# preds4 = pd.Series(model4.predict(nn_train[0]).reshape(-1)).clip(0, 6.8).values\n",
    "\n",
    "# print('RMSE GradientBoostingRegressor: ', RMSLE(np.log1p(train['visitors'].values), preds1))\n",
    "# print('RMSE KNeighborsRegressor: ', RMSLE(np.log1p(train['visitors'].values), preds2))\n",
    "# print('RMSE XGBRegressor: ', RMSLE(np.log1p(train['visitors'].values), preds3))\n",
    "# print('RMSE RamdomForestRegressor: ', RMSLE(np.log1p(train['visitors'].values), preds5))\n",
    "# print('RMSE NeuralNetwork: ', RMSLE(np.log1p(train['visitors'].values), preds4))\n",
    "\n",
    "preds1 = model1.predict(test[col])\n",
    "preds2 = model2.predict(test[col])\n",
    "preds3 = model3.predict(test[col])\n",
    "preds5 = model5.predict(test[col])\n",
    "# .clip(0, 6.8) used to avoid random high values that might occur\n",
    "preds4 = pd.Series(model4.predict(nn_test[0]).reshape(-1)).clip(0, 6.8).values\n",
    "\n",
    "# test['visitors'] = 0.2*preds1+0.2*preds2+0.3*preds3+0.3*preds4\n",
    "test['visitors'] = 0.2*preds1\n",
    "+0.1*preds2\n",
    "+0.25*preds3\n",
    "+0.45*preds4\n",
    "# +0.1*preds5\n",
    "test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\n",
    "sub1 = test[['id','visitors']].copy()\n",
    "print(\"Model predictions done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../../data2/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub2 = sub2.fillna(-1) # for the unfound values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def final_visitors(x, alt=False):\n",
    "    visitors_x, visitors_y = x['visitors_x'], x['visitors_y']\n",
    "    if x['visitors_y'] == -1:\n",
    "        return visitors_x\n",
    "    else:\n",
    "        return 0.7*visitors_x + 0.3*visitors_y* 1.1\n",
    "\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')\n",
    "sub_merge['visitors'] = sub_merge.apply(lambda x: final_visitors(x), axis=1)\n",
    "print(\"Done\")\n",
    "sub_merge[['id', 'visitors']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# test['visitors'] = (\n",
    "#     model1.predict(test[col]) +\n",
    "#     model2.predict(test[col]) + \n",
    "#                     model3.predict(test[col])\n",
    "# #     + \n",
    "# #                     model4.predict(test[col])\n",
    "# ) / 3\n",
    "# # test['visitors'] = (model1.predict(test[col]) + model2.predict(test[col])) / 2\n",
    "# test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\n",
    "# sub1 = test[['id','visitors']].copy()\n",
    "# # del train; del data;\n",
    "\n",
    "# # from hklee\n",
    "# # https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "# dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "#     pd.read_csv(fn)for fn in glob.glob('../../data2/*.csv')}\n",
    "\n",
    "# for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "# wkend_holidays = date_info.apply(\n",
    "#     (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "# date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "# date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "# visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "# visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "# visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "# wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "# visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "# visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "# sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "# sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "# sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "# sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "# sample_submission = sample_submission.merge(visitors, on=[\n",
    "#     'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "# missings = sample_submission.visitors.isnull()\n",
    "# sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "#     visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "#     how='left')['visitors_y'].values\n",
    "\n",
    "# missings = sample_submission.visitors.isnull()\n",
    "# sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "#     visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "#     on='air_store_id', how='left')['visitors_y'].values\n",
    "\n",
    "# sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "# sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "# sub_merge = pd.merge(sub1, sub2, on='id', how='inner')\n",
    "\n",
    "# sub_merge['visitors'] = (sub_merge['visitors_x'] + sub_merge['visitors_y']* 1.1)/2\n",
    "\n",
    "# sub_merge[['id', 'visitors']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
